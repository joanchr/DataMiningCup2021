# -*- coding: utf-8 -*-
"""PCA_Transactions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWY94033thKITLwuCk0tTgCzt53ZQA3f

# Import
"""

from pyspark.sql import SparkSession

spark_session = SparkSession.builder.getOrCreate()

"""# Read Data"""

transactions = spark_session.read.option("inferSchema","true").csv("transactions.csv", header=True, sep="|")

transactions.show(3)

"""# Data Preprocessing

### Add interaction level (binary, categorical, and weight based)
"""

from pyspark.sql.functions import when,count

transactions = transactions.withColumn(
    "binary",
    when(transactions["click"] == 1, 1).
    when(transactions["basket"] == 1, 1).
    when(transactions["order"] == 1, 1).
    otherwise(0)
)

transactions = transactions.withColumn(
    "categorical",
    when(transactions["click"] == 1, 1).
    when(transactions["basket"] == 1, 2).
    when(transactions["order"] == 1, 3).
    otherwise(0)
)

transactions = transactions.withColumn(
    "weight",
    (transactions["click"] * 1) + (transactions["basket"] * 2) + (transactions["order"] * 3)
)

transactions.show(10)

"""### Split into 3 datasets"""

transactions_binary = transactions.select("sessionID","itemID","binary")
transactions_categorical = transactions.select("sessionID","itemID","categorical")
transactions_weight = transactions.select("sessionID","itemID","weight")

transactions_binary.show(3)
transactions_categorical.show(3)
transactions_weight.show(3)

"""### Pivot"""

from pyspark.sql.functions import sum

spark_session.conf.set("spark.sql.pivotMaxValues",25000)

"""cara pake pivot:
https://stackoverflow.com/questions/46809879/convert-pyspark-groupeddata-object-to-spark-dataframe
1. groupBy = column
2. pivot = row
3. agg = cell -> bisa juga pake .count()
"""

pivot_binary = transactions_binary.groupBy("itemID").pivot("sessionID").agg(sum("binary"))

pivot_categorical = transactions_categorical.groupBy("itemID").pivot("sessionID").agg(sum("categorical"))

pivot_weight = transactions_weight.groupBy("itemID").pivot("sessionID").agg(sum("weight"))

pivot_binary = pivot_binary.fillna(0)

pivot_categorical = pivot_categorical.fillna(0)

pivot_weight = pivot_weight.fillna(0)

pivot_binary.show()

"""### Dimensionality Reduction"""

from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg.distributed import RowMatrix

mat_binary = pivot_binary.drop("itemID").rdd.map(lambda s : Vectors.dense(s))

mat_categorical = pivot_categorical.drop("itemID").rdd.map(lambda s : Vectors.dense(s))

mat_weight = pivot_weight.drop("itemID").rdd.map(lambda s : Vectors.dense(s))

mat_binary = RowMatrix(mat_binary)

mat_categorical = RowMatrix(mat_categorical)

mat_weight = RowMatrix(mat_weight)

"""### Principal component analysis (PCA)"""

pca_binary = mat_binary.computePrincipalComponents(5)

pca_categorical = mat_categorical.computePrincipalComponents(5)

pca_weight = mat_weight.computePrincipalComponents(5)

projected_binary = mat_binary.multiply(pca_binary)

projected_categorical = mat_binary.multiply(pca_categorical)

projected_weight = mat_binary.multiply(pca_weight)

print(projected_binary.rows.collect())

projected_binary.numRows()

projected_binary.numCols()

print(projected_categorical.rows.collect())

print(projected_weight.rows.collect())